{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabio\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import torch\n",
    "import json\n",
    "import torch\n",
    "import ast\n",
    "import uuid\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "from openai import OpenAI\n",
    "torch.manual_seed(0)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria, StoppingCriteriaList\n",
    "from guidance import models, select, gen\n",
    "global model\n",
    "global modelType\n",
    "global inferenceType\n",
    "global tokenizer\n",
    "global LLM\n",
    "\n",
    "\n",
    "global promptModel\n",
    "global promptModelType\n",
    "global promptInferenceType\n",
    "global promptModelTokenizer\n",
    "global promptLLM\n",
    "global previous_results\n",
    "global topics\n",
    "global selectOptions\n",
    "global topic_id_counter\n",
    "global interface\n",
    "\n",
    "\n",
    "model=\"\"\n",
    "modelType=\"Transformers\"\n",
    "inferenceType=\"\"\n",
    "promptModel=\"\"\n",
    "promptModelType=\"Transformers\"\n",
    "promptInferenceType=\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "topics = []\n",
    "\n",
    "\n",
    "interface = False\n",
    "previous_results = {}\n",
    "topic_id_counter = 0\n",
    "\n",
    "\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, stop_token_ids):\n",
    "        self.stop_token_ids = stop_token_ids\n",
    "    \n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        for stop_id in self.stop_token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def setModel(newModel,newModelType=\"Transformers\",api_key=\"\",newInferenceType=\"transformers\"):\n",
    "    global model\n",
    "    global modelType\n",
    "    global inferenceType\n",
    "    model=newModel\n",
    "    modelType=newModelType\n",
    "    inferenceType=newInferenceType\n",
    "    global ModelGuidance \n",
    "    global client\n",
    "    global LLM\n",
    "    global tokenizer\n",
    "    if modelType==\"Transformers\":\n",
    "        if inferenceType==\"transformers\":\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=True)\n",
    "            LLM = AutoModelForCausalLM.from_pretrained(\n",
    "                model,\n",
    "                trust_remote_code=True,\n",
    "                # attn_implementation = \"flash_attention_2\"\n",
    "            )\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        elif inferenceType==\"guidance\":\n",
    "            ModelGuidance = models.Transformers(model, echo=False, trust_remote_code=True)\n",
    "        else:\n",
    "            print(\"Invalid inference Type.\")\n",
    "        \n",
    "    if modelType==\"OpenAI\":\n",
    "        inferenceType=\"cloud\"\n",
    "        if not api_key==\"\":\n",
    "            client = OpenAI(api_key=api_key)\n",
    "    if modelType==\"DeepInfra\":\n",
    "        inferenceType=\"cloud\"\n",
    "        if not api_key==\"\":\n",
    "            client = OpenAI(api_key=api_key,base_url=\"https://api.deepinfra.com/v1/openai\")\n",
    "\n",
    "            \n",
    "def load_model():\n",
    "    \"\"\"Load model and return it\"\"\"\n",
    "    LLM = AutoModelForCausalLM.from_pretrained(\n",
    "        model,\n",
    "        device_map='cuda',\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"eager\"\n",
    "    )        \n",
    "    return LLM\n",
    "    \n",
    "def unload_model(model_to_unload):\n",
    "    \"\"\"Thoroughly unload model from GPU memory\"\"\"\n",
    "    if model_to_unload is not None:\n",
    "        # Move model to CPU first\n",
    "        model_to_unload.cpu()\n",
    "        # Clear CUDA cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        # Delete the model object\n",
    "        del model_to_unload\n",
    "        # Force garbage collection multiple times\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    return None\n",
    "\n",
    "\n",
    "def setPromptModel(newPromptModel, newPromptModelType, api_key=\"\", newInferenceType=\"guidance\"):\n",
    "    global Model\n",
    "    global promptModel\n",
    "    global promptModelType\n",
    "    global promptInferenceType\n",
    "    promptModel=newPromptModel\n",
    "    promptModelType=newPromptModelType\n",
    "    promptInferenceType=newInferenceType\n",
    "    global promptModelGuidance \n",
    "    global promptLLM\n",
    "    global client\n",
    "    if promptModelType==\"Transformers\":\n",
    "        if inferenceType==\"transformers\":\n",
    "            promptModelTokenizer = AutoTokenizer.from_pretrained(promptModel, trust_remote_code=True)\n",
    "            promptLLM = AutoModelForCausalLM.from_pretrained(\n",
    "                promptModel,\n",
    "                device_map='cuda',\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                trust_remote_code=True\n",
    "                # attn_implementation = \"flash_attention_2\"\n",
    "            )\n",
    "            promptModelTokenizer.pad_token_id = promptModelTokenizer.eos_token_id\n",
    "        elif inferenceType==\"guidance\":\n",
    "            if promptModel==Model:\n",
    "                promptModelGuidance=ModelGuidance\n",
    "            else:\n",
    "                promptModelGuidance = models.Transformers(model, device_map='cuda', torch_dtype=torch.bfloat16, echo=False, trust_remote_code=True)\n",
    "    if modelType==\"OpenAI\" or promptModelType==\"OpenAI\":\n",
    "        if not api_key==\"\":\n",
    "            client = OpenAI(api_key=api_key)\n",
    "    if modelType==\"DeepInfra\" or promptModelType==\"DeepInfra\":\n",
    "        if not api_key==\"\":\n",
    "            client = OpenAI(api_key=api_key,base_url=\"https://api.deepinfra.com/v1/openai\")\n",
    "\n",
    "\n",
    "def calculate_word_probability(prompt, target_word):\n",
    "    \"\"\"\n",
    "    Only available for inferenceType='transformers'\n",
    "    Calculate the probability of a target word following a prompt.\n",
    "\n",
    "    Args:\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        prompt: Input prompt string\n",
    "        target_word: The word to calculate probability for\n",
    "\n",
    "    Returns:\n",
    "        tuple: (probability, token_probabilities)\n",
    "    \"\"\"\n",
    "\n",
    "    LLM.eval()\n",
    "\n",
    "    if not prompt.endswith(' '):\n",
    "        target_word = ' ' + target_word\n",
    "\n",
    "    target_tokens = tokenizer.encode(target_word, add_special_tokens=False)\n",
    "    token_probabilities = []\n",
    "    current_text = prompt\n",
    "\n",
    "    for i, token_id in enumerate(target_tokens):\n",
    "        inputs = tokenizer(current_text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = LLM(**inputs)\n",
    "        next_token_logits = outputs.logits[0, -1, :]\n",
    "        next_token_probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "        token_prob = next_token_probs[token_id].item()\n",
    "        token_probabilities.append(token_prob)\n",
    "        current_text = tokenizer.decode(\n",
    "            tokenizer.encode(current_text, add_special_tokens=False) + [token_id],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "    total_probability = torch.tensor(token_probabilities).prod().item()\n",
    "\n",
    "    return total_probability, token_probabilities\n",
    "    \n",
    "    \n",
    "def calculate_options_probabilities(prompt, options):\n",
    "    \"\"\"   \n",
    "    Args:\n",
    "        prompt: Input prompt string\n",
    "        options: List of possible options to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (best_option, best_probability, all_probabilities_dict, relative_probabilities_dict)\n",
    "    \"\"\"\n",
    "    LLM.eval()\n",
    "    if not prompt.endswith(' '):\n",
    "        space_prefix = ' '\n",
    "    else:\n",
    "        space_prefix = ''\n",
    "    first_token_groups = {}\n",
    "    for option in options:\n",
    "        first_token = tokenizer.encode(space_prefix + option, add_special_tokens=False)[0]\n",
    "        if first_token not in first_token_groups:\n",
    "            first_token_groups[first_token] = []\n",
    "        first_token_groups[first_token].append(option)\n",
    "    base_inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    option_probabilities = {}\n",
    "    with torch.no_grad():\n",
    "        base_outputs = LLM(**base_inputs)\n",
    "        base_logits = base_outputs.logits[0, -1, :]\n",
    "        base_probs = torch.nn.functional.softmax(base_logits, dim=-1)\n",
    "        \n",
    "    for first_token_id, group_options in first_token_groups.items():\n",
    "        if len(group_options) == 1:\n",
    "            option = group_options[0]\n",
    "            option_with_space = space_prefix + option\n",
    "            token_ids = tokenizer.encode(option_with_space, add_special_tokens=False)\n",
    "            \n",
    "            if len(token_ids) == 1:\n",
    "                option_probabilities[option] = base_probs[first_token_id].item()\n",
    "            else:\n",
    "                probability, _ = calculate_word_probability(prompt, option)\n",
    "                option_probabilities[option] = probability\n",
    "        else:\n",
    "            for option in group_options:\n",
    "                probability, _ = calculate_word_probability(prompt, option)\n",
    "                option_probabilities[option] = probability\n",
    "    \n",
    "    total_probability = sum(option_probabilities.values())\n",
    "    relative_probabilities = {\n",
    "        option: (prob / total_probability if total_probability > 0 else 0)\n",
    "        for option, prob in option_probabilities.items()\n",
    "    }\n",
    "    \n",
    "    best_option = max(option_probabilities.items(), key=lambda x: x[1])\n",
    "    \n",
    "    return best_option[0], best_option[1], option_probabilities, relative_probabilities, relative_probabilities[best_option[0]]\n",
    "     \n",
    "    \n",
    "        \n",
    "def getAnswer(prompt, categories, constrainedOutput, temperature=0.0, thinkStep=0):\n",
    "    if inferenceType==\"cloud\":\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=30,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        generated_answer = completion.choices[0].message.content\n",
    "        for option in categories:\n",
    "            escaped_option = re.escape(option)\n",
    "\n",
    "            if re.search(escaped_option, generated_answer, re.IGNORECASE):\n",
    "                ret = option  \n",
    "                break\n",
    "        else:\n",
    "            ret = \"undefined\"\n",
    "        return ret, \"-\"\n",
    "        \n",
    "    elif inferenceType==\"guidance\":\n",
    "        if constrainedOutput==True:\n",
    "            output=ModelGuidance+f' '+prompt+select(options=categories,name='answer')\n",
    "            ret=output[\"answer\"]   \n",
    "        else:\n",
    "            output=ModelGuidance+f' '+prompt+gen(max_tokens=15,name='answer')\n",
    "            generated_answer = output[\"answer\"]\n",
    "            for option in categories:\n",
    "                escaped_option = re.escape(option)\n",
    "                if re.search(escaped_option, generated_answer, re.IGNORECASE):\n",
    "                    ret = option  \n",
    "                    break\n",
    "            else:\n",
    "                ret = \"undefined\"\n",
    "\n",
    "\n",
    "        return ret, \"-\"\n",
    "    elif inferenceType==\"transformers\":\n",
    "        ### Optional additional Think-Step\n",
    "        if thinkStep>0:\n",
    "            current_text = prompt\n",
    "            inputs = tokenizer(current_text, return_tensors=\"pt\")\n",
    "            period_id = tokenizer.encode(\".\", add_special_tokens=False)[-1]\n",
    "            question_id = tokenizer.encode(\"?\", add_special_tokens=False)[-1]\n",
    "            exclamation_id = tokenizer.encode(\"!\", add_special_tokens=False)[-1]\n",
    "            stopping_criteria = StoppingCriteriaList([\n",
    "            StopOnTokens([period_id, question_id, exclamation_id])\n",
    "            ])\n",
    "        \n",
    "            outputs = LLM.generate(inputs.input_ids, temperature=0.01, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95, stopping_criteria=stopping_criteria)\n",
    "            newPrompt=tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]+\" Therefore the correct answer is '\"\n",
    "            print(\"New Prompt after Think-Step: \", newPrompt)\n",
    "        else:\n",
    "            newPrompt=prompt\n",
    "        if constrainedOutput==True:\n",
    "            best_option, best_prob, all_probs_abs, all_probs_rel, best_rel_prob = calculate_options_probabilities(newPrompt, categories)\n",
    "        else:\n",
    "            inputs = tokenizer(newPrompt, return_tensors=\"pt\")\n",
    "            outputs = LLM.generate(inputs.input_ids, max_length=100, num_return_sequences=1, temperature=temperature, do_sample=True, top_k=50, top_p=0.95)\n",
    "            for option in categories:\n",
    "                escaped_option = re.escape(option)\n",
    "                if re.search(escaped_option, generated_answer, re.IGNORECASE):\n",
    "                    ret = option  \n",
    "                    break\n",
    "                else:\n",
    "                    ret = \"undefined\"\n",
    "            best_option=ret\n",
    "            best_rel_prob=\"-\"\n",
    "            \n",
    "        return best_option, best_rel_prob\n",
    "    \n",
    "\n",
    "def evaluate_condition(condition):\n",
    "    if not condition: \n",
    "        return True\n",
    "\n",
    "    if \"==\" not in condition:\n",
    "        print(f\"Invalid condition format: {condition}\")\n",
    "        return False\n",
    "\n",
    "    left_side, right_side = condition.split(\"==\", 1)\n",
    "    left_side = left_side.strip()\n",
    "    right_side = right_side.strip()\n",
    "\n",
    "    if left_side not in previous_results:\n",
    "        #print(f\"No previous classification result for topic '{left_side}'. Condition: {condition}\")\n",
    "        return False\n",
    "\n",
    "    chosen_cat_id = previous_results[left_side]\n",
    "\n",
    "    return chosen_cat_id == right_side\n",
    "\n",
    "\n",
    "def classify(text, isItASingleClassification=True, constrainedOutput=True, withEvaluation=False, groundTruthRow=None):\n",
    "    selectOptions = []\n",
    "    for topic_data in topics:\n",
    "        tmpSelectOptions = \"[\"\n",
    "        for category_input, _, _ in topic_data['categories']:\n",
    "            tmpSelectOptions += \"'\" + category_input.value + \"',\"\n",
    "        tmpSelectOptions = tmpSelectOptions[:-1] + \"]\"\n",
    "        selectOptions.append(tmpSelectOptions)\n",
    "\n",
    "    ret = []\n",
    "    probs = []\n",
    "\n",
    "    if withEvaluation and groundTruthRow is not None:\n",
    "        for i, topic_info in enumerate(topics):\n",
    "            groundTruthCategoryName = groundTruthRow[i+1] \n",
    "            gt_cat_id = None\n",
    "            for (cat_input, _, cat_id) in topic_info['categories']:\n",
    "                if cat_input.value == groundTruthCategoryName:\n",
    "                    gt_cat_id = cat_id\n",
    "                    break\n",
    "            previous_results[topic_info['id']] = gt_cat_id\n",
    "\n",
    "    for l in range(len(selectOptions)):\n",
    "        condition = topics[l]['condition'].value.strip()\n",
    "        condition_is_true = evaluate_condition(condition)\n",
    "\n",
    "        if not condition_is_true:\n",
    "            ret.append(\"\")\n",
    "            if interface == True and isItASingleClassification:\n",
    "                print(f\"Skipping {topics[l]['topic_input'].value} due to unmet condition: {condition}\")\n",
    "            continue\n",
    "\n",
    "        prompt = topics[l]['prompt'].value\n",
    "        prompt = prompt.replace('[TOPIC]', topics[l]['topic_input'].value)\n",
    "        prompt = prompt.replace('[CATEGORIES]', selectOptions[l])\n",
    "        prompt = prompt.replace('[TEXT]', text)\n",
    "\n",
    "        categories=ast.literal_eval(selectOptions[l])\n",
    "        answer, bestRelProb = getAnswer(prompt, categories, constrainedOutput, 0.0)\n",
    "        \n",
    "        #Output for Single Classification\n",
    "        if isItASingleClassification==True:\n",
    "            print(topics[l]['topic_input'].value,\":\",answer,\" (Relative Probability:\",bestRelProb,\")\")\n",
    "        ret.append(answer)\n",
    "        probs.append(bestRelProb)\n",
    "\n",
    "        if not withEvaluation:\n",
    "            chosen_category_id = None\n",
    "            for category_input, _, category_id in topics[l]['categories']:\n",
    "                if category_input.value == answer:\n",
    "                    chosen_category_id = category_id\n",
    "                    break\n",
    "            previous_results[topics[l]['id']] = chosen_category_id\n",
    "\n",
    "        if interface == True and isItASingleClassification:\n",
    "            print(f\"{topics[l]['topic_input'].value}: {answer}\")\n",
    "\n",
    "    return ret,probs\n",
    "\n",
    "        \n",
    "def get_current_accuracy(topic_info):\n",
    "    label_text = topic_info['performance_label'].value\n",
    "    match = re.match(r\"Accuracy:\\s+([\\d.]+)%\", label_text)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "def generate_id():\n",
    "    return str(uuid.uuid4())[:8] \n",
    "\n",
    "\n",
    "\n",
    "def number_to_letters(num, uppercase=True):\n",
    "    letters = \"\"\n",
    "    while num > 0:\n",
    "        num -= 1\n",
    "        letters = chr((num % 26) + (65 if uppercase else 97)) + letters\n",
    "        num //= 26\n",
    "    return letters\n",
    "\n",
    "\n",
    "\n",
    "def show_topics_and_categories():\n",
    "    if not topics:\n",
    "        print(\"No topics are currently defined.\")\n",
    "        return\n",
    "\n",
    "    for i, topic_info in enumerate(topics, start=1):\n",
    "        topic_name = topic_info['topic_input'].value\n",
    "        topic_id = topic_info.get('id', '?')\n",
    "        \n",
    "        condition_val = topic_info['condition'].value if 'condition' in topic_info else None\n",
    "        prompt_val    = topic_info['prompt'].value    if 'prompt'    in topic_info else None\n",
    "        \n",
    "        print(f\"Topic {i} (ID={topic_id}): {topic_name}\")\n",
    "\n",
    "        if condition_val:\n",
    "            print(f\"  Condition: {condition_val}\")\n",
    "\n",
    "        if prompt_val:\n",
    "            print(f\"  Prompt: {prompt_val}\")\n",
    "\n",
    "        categories = topic_info.get('categories', [])\n",
    "        if not categories:\n",
    "            print(\"    [No categories in this topic]\")\n",
    "        else:\n",
    "            for j, (category_input, _, cat_id) in enumerate(categories, start=1):\n",
    "                cat_name = category_input.value\n",
    "                print(f\"    {j}. {cat_name} (ID={cat_id})\")\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "def add_topic(topic_name, \n",
    "              categories=[], \n",
    "              condition=\"\", \n",
    "              prompt=\"INSTRUCTION: You are a helpful classifier. You select the correct of the possible categories \"\n",
    "        \"for classifying a piece of text. The topic of the classification is '[TOPIC]'. \"\n",
    "        \"The allowed categories are '[CATEGORIES]'. QUESTION: The text to be classified is '[TEXT]'. \"\n",
    "        \"ANSWER: The correct category for this text is '\"):\n",
    "   \n",
    "    global topic_id_counter\n",
    "    topic_id_counter += 1\n",
    "    \n",
    "    if prompt is None:\n",
    "        prompt = (\n",
    "            \"INSTRUCTION: You are a helpful classifier. You select the correct of the possible categories \"\n",
    "            \"for classifying a piece of text. The topic of the classification is '[TOPIC]'. \"\n",
    "            \"The allowed categories are '[CATEGORIES]'. QUESTION: The text to be classified is '[TEXT]'. \"\n",
    "            \"ANSWER: The correct category for this text is '\"\n",
    "        )\n",
    "\n",
    "    topic_input_mock = MockText(topic_name)\n",
    "    condition_mock = MockText(condition)\n",
    "    prompt_mock = MockText(prompt)\n",
    "    \n",
    "    topic_id = number_to_letters(topic_id_counter, uppercase=True)\n",
    "    \n",
    "    topic_info = {\n",
    "        'id': topic_id,\n",
    "        'topic_input': topic_input_mock,\n",
    "        'condition': condition_mock,\n",
    "        'categories': [],\n",
    "        'prompt': prompt_mock,\n",
    "        'categories_container': None,\n",
    "        'topic_box': None,\n",
    "        'performance_label': None,\n",
    "        'checkPrompt_button': None,\n",
    "        'num_iterations_input': None,\n",
    "        'iteratePromptImprovements_button': None,\n",
    "        'replacePrompt_button': None,\n",
    "        \n",
    "        'best_prompt_found': None,\n",
    "        'best_prompt_accuracy': None,\n",
    "        \n",
    "        'category_counter': 0\n",
    "    }\n",
    "    \n",
    "    for cat_str in categories:\n",
    "        topic_info['category_counter'] += 1\n",
    "        cat_id = number_to_letters(topic_info['category_counter'], uppercase=False)  # a, b, c ...\n",
    "        category_tuple = (MockText(cat_str), None, cat_id)\n",
    "        \n",
    "        topic_info['categories'].append(category_tuple)\n",
    "    \n",
    "    topics.append(topic_info)\n",
    "    return topic_info\n",
    "\n",
    "\n",
    "def remove_topic(topic_id_str):\n",
    "    for i, t in enumerate(topics):\n",
    "        if t.get('id') == topic_id_str:\n",
    "            del topics[i]\n",
    "            print(f\"Topic (ID={topic_id_str}) removed.\")\n",
    "            return \n",
    "\n",
    "    print(f\"No topic found with ID={topic_id_str}.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "def add_category(topicId, categoryName, Condition=\"\"):\n",
    "    found_topic = None\n",
    "    for topic_info in topics:\n",
    "        if topic_info.get('id') == topicId:\n",
    "            found_topic = topic_info\n",
    "            break\n",
    "\n",
    "    if not found_topic:\n",
    "        print(f\"No topic found with ID={topicId}\")\n",
    "        return\n",
    "\n",
    "    if 'category_counter' not in found_topic:\n",
    "        found_topic['category_counter'] = 0\n",
    "\n",
    "    found_topic['category_counter'] += 1\n",
    "    cat_id = number_to_letters(found_topic['category_counter'], uppercase=False)\n",
    "    new_category_tuple = (MockText(categoryName), None, cat_id)\n",
    "\n",
    "    if 'categories' not in found_topic:\n",
    "        found_topic['categories'] = []\n",
    "    found_topic['categories'].append(new_category_tuple)\n",
    "\n",
    "    if Condition:\n",
    "        if 'condition' not in found_topic or not hasattr(found_topic['condition'], 'value'):\n",
    "            found_topic['condition'] = MockText(\"\")\n",
    "        found_topic['condition'].value = Condition\n",
    "\n",
    "    print(f\"Category '{categoryName}' (ID={cat_id}) added to topic '{topicId}'.\")\n",
    "    if Condition:\n",
    "        print(f\"  Updated topic condition to: {Condition}\")\n",
    "        \n",
    "        \n",
    "def remove_category(topicId, categoryId):\n",
    "    for topic_info in topics:\n",
    "        if topic_info.get('id') == topicId:\n",
    "            categories = topic_info.get('categories', [])\n",
    "            for i, (cat_input, cat_box, cat_id) in enumerate(categories):\n",
    "                if cat_id == categoryId:\n",
    "                    del categories[i]\n",
    "                    print(f\"Removed category (ID={categoryId}) from topic (ID={topicId}).\")\n",
    "                    return\n",
    "            \n",
    "            print(f\"Category with ID='{categoryId}' not found in topic (ID={topicId}).\")\n",
    "            return\n",
    "\n",
    "    print(f\"No topic found with ID='{topicId}'.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "def save_topics(filename):\n",
    "    data = []\n",
    "    for topic_info in topics:\n",
    "        topic_data = {\n",
    "            'id': topic_info.get('id', ''),\n",
    "            'topic_input': topic_info['topic_input'].value if 'topic_input' in topic_info else '',\n",
    "            'condition': topic_info['condition'].value if 'condition' in topic_info else '',\n",
    "            'prompt': topic_info['prompt'].value if 'prompt' in topic_info else '',\n",
    "            'categories': []\n",
    "        }\n",
    "\n",
    "        for (cat_input, _, cat_id) in topic_info.get('categories', []):\n",
    "            cat_name = cat_input.value\n",
    "            topic_data['categories'].append({\n",
    "                'id': cat_id,\n",
    "                'value': cat_name\n",
    "            })\n",
    "\n",
    "        data.append(topic_data)\n",
    "\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Topics saved to {filename}\")\n",
    "    \n",
    "def load_topics(filename):\n",
    "    global topics\n",
    "    topics.clear()  \n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"File not found: {filename}\")\n",
    "        return\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for topic_data in data:\n",
    "        new_topic = {\n",
    "            'id': topic_data.get('id', ''),\n",
    "            'topic_input': MockText(topic_data.get('topic_input', '')),\n",
    "            'condition': MockText(topic_data.get('condition', '')),\n",
    "            'prompt': MockText(topic_data.get('prompt', '')),\n",
    "            'categories': [],\n",
    "            'category_counter': 0\n",
    "        }\n",
    "\n",
    "        for cat_dict in topic_data.get('categories', []):\n",
    "            cat_id = cat_dict.get('id', '')\n",
    "            cat_value = cat_dict.get('value', '')\n",
    "            new_topic['category_counter'] += 1\n",
    "            new_topic['categories'].append(\n",
    "                (MockText(cat_value), None, cat_id)\n",
    "            )\n",
    "\n",
    "        topics.append(new_topic)\n",
    "\n",
    "    print(f\"Loaded {len(topics)} topic(s) from {filename}\")\n",
    "    \n",
    "    \n",
    "def add_condition(topicId, categoryId, conditionStr):\n",
    "    found_topic = None\n",
    "    for topic in topics:\n",
    "        if topic.get('id') == topicId:\n",
    "            found_topic = topic\n",
    "            break\n",
    "\n",
    "    if found_topic is None:\n",
    "        print(f\"No topic found with ID={topicId}.\")\n",
    "        return\n",
    "\n",
    "    categories = found_topic.get('categories', [])\n",
    "    \n",
    "    for i, cat_tuple in enumerate(categories):\n",
    "        if len(cat_tuple) == 3:\n",
    "            (cat_input, cat_box, cat_id) = cat_tuple\n",
    "            cat_condition = \"\"  # no condition yet\n",
    "        else:\n",
    "            (cat_input, cat_box, cat_id, cat_condition) = cat_tuple\n",
    "\n",
    "        if cat_id == categoryId:\n",
    "            new_cat_tuple = (cat_input, cat_box, cat_id, conditionStr)\n",
    "            categories[i] = new_cat_tuple\n",
    "            print(f\"Condition '{conditionStr}' added to category (ID={categoryId}) in topic (ID={topicId}).\")\n",
    "            return\n",
    "\n",
    "    print(f\"No category (ID={categoryId}) found in topic (ID={topicId}).\")\n",
    "    \n",
    "    \n",
    "def remove_condition(topicId, categoryId):\n",
    "    found_topic = None\n",
    "    for topic in topics:\n",
    "        if topic.get('id') == topicId:\n",
    "            found_topic = topic\n",
    "            break\n",
    "\n",
    "    if found_topic is None:\n",
    "        print(f\"No topic found with ID={topicId}.\")\n",
    "        return\n",
    "\n",
    "    categories = found_topic.get('categories', [])\n",
    "\n",
    "    for i, cat_tuple in enumerate(categories):\n",
    "        if len(cat_tuple) == 3:\n",
    "            (cat_input, cat_box, cat_id) = cat_tuple\n",
    "            cat_condition = None  \n",
    "        else:\n",
    "            (cat_input, cat_box, cat_id, cat_condition) = cat_tuple\n",
    "\n",
    "        if cat_id == categoryId:\n",
    "            if len(cat_tuple) == 3:\n",
    "                print(f\"Category (ID={categoryId}) in topic (ID={topicId}) has no condition.\")\n",
    "                return\n",
    "            else:\n",
    "                new_cat_tuple = (cat_input, cat_box, cat_id, \"\")\n",
    "                categories[i] = new_cat_tuple\n",
    "                print(f\"Condition removed from category (ID={categoryId}) in topic (ID={topicId}).\")\n",
    "                return\n",
    "\n",
    "    print(f\"No category (ID={categoryId}) found in topic (ID={topicId}).\")\n",
    "\n",
    "    \n",
    "    \n",
    "def get_header_list():\n",
    "    global topics\n",
    "    headerlist=[\"Text\"]\n",
    "    for l in range(len(topics)):\n",
    "        headerlist.append(topics[l]['topic_input'].value)\n",
    "    return headerlist\n",
    "\n",
    "    \n",
    "def classify_table(dataset, withEvaluation=False, constrainedOutput=True, BATCH_SIZE=100):\n",
    "    global LLM\n",
    "    start_time = time.time()\n",
    "    csv_file = dataset + \".csv\"\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"No {csv_file} file found.\")\n",
    "        return\n",
    "\n",
    "    categoryConfusions = []\n",
    "    for i, topic_info in enumerate(topics):\n",
    "        cat_map = {}\n",
    "        for (cat_input, _, _cat_id) in topic_info['categories']:\n",
    "            cat_name = cat_input.value\n",
    "            cat_map[cat_name] = {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"TN\": 0}\n",
    "        categoryConfusions.append(cat_map)\n",
    "\n",
    "    numberOfCorrectResults = []\n",
    "    numberOfRelevantAttempts = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    def process_batch(batch_rows, start_index):\n",
    "        global LLM\n",
    "        \"\"\"Process a batch of rows with given model\"\"\"\n",
    "        batch_results = []\n",
    "        batch_probs = []\n",
    "        batch_indices = []\n",
    "        for idx, row in enumerate(batch_rows):\n",
    "            actual_index = start_index + idx\n",
    "            if withEvaluation:\n",
    "                result, probs = classify(\n",
    "                    row[0],\n",
    "                    isItASingleClassification=False,\n",
    "                    constrainedOutput=constrainedOutput,\n",
    "                    withEvaluation=True,\n",
    "                    groundTruthRow=row\n",
    "                )\n",
    "            else:\n",
    "                result, probs = classify(\n",
    "                    row[0],\n",
    "                    isItASingleClassification=False,\n",
    "                    constrainedOutput=constrainedOutput\n",
    "                )\n",
    "            batch_results.append(result)\n",
    "            batch_probs.append(probs)\n",
    "            batch_indices.append(actual_index)\n",
    "        return batch_results, batch_probs, batch_indices\n",
    "\n",
    "    startcount = 1\n",
    "    endcount = -1\n",
    "    saveName = dataset + \"_(result)\"\n",
    "\n",
    "    with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file, delimiter=';')\n",
    "        readerlist = list(reader)\n",
    "\n",
    "        header_row = get_header_list()\n",
    "        singleResult = [\"\"]\n",
    "        elementcounter = -1\n",
    "        for element in header_row:\n",
    "            elementcounter += 1\n",
    "            if elementcounter == 0:\n",
    "                singleResult.append(element)\n",
    "            else:\n",
    "                numberOfCorrectResults.append(0)\n",
    "                numberOfRelevantAttempts.append(0)\n",
    "                if withEvaluation:\n",
    "                    singleResult.append(element + \"(GroundTruth)\")\n",
    "                singleResult.append(element)\n",
    "                singleResult.append(\"Probability\")\n",
    "\n",
    "        with open(saveName + \".csv\", 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f, delimiter=';', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "            writer.writerow(singleResult)\n",
    "\n",
    "        # Process data rows in batches with model reloading\n",
    "        current_batch = []\n",
    "        total_rows = len(readerlist[startcount:endcount if endcount != -1 else None])\n",
    "        processed_rows = 0\n",
    "        current_index = startcount\n",
    "                \n",
    "        \n",
    "        for count, row in enumerate(readerlist[startcount:endcount if endcount != -1 else None], start=startcount):\n",
    "            current_batch.append(row)\n",
    "            \n",
    "            if len(current_batch) >= BATCH_SIZE or processed_rows + len(current_batch) == total_rows:\n",
    "                \n",
    "                \n",
    "                # Process batch\n",
    "                batch_results, batch_probs, batch_indices = process_batch(current_batch, current_index)\n",
    "                \n",
    "                # Unload model to free GPU memory\n",
    "                if(modelType==\"Transformers\"):\n",
    "                    unload_model(LLM)\n",
    "                \n",
    "                    # Load fresh model instance\n",
    "                    LLM = load_model()\n",
    "                \n",
    "                # Write results and update metrics\n",
    "                for idx, (batch_row, result, prob) in enumerate(zip(current_batch, batch_results, batch_probs)):\n",
    "                    # Use the correct row index from batch_indices\n",
    "                    row_index = batch_indices[idx]\n",
    "                    process_and_write_result(\n",
    "                        row_index, batch_row, result, prob, \n",
    "                        saveName, withEvaluation, \n",
    "                        numberOfCorrectResults, \n",
    "                        numberOfRelevantAttempts,\n",
    "                        categoryConfusions\n",
    "                    )\n",
    "                \n",
    "                processed_rows += len(current_batch)\n",
    "                print(f\"Processed {processed_rows}/{total_rows} rows ({(processed_rows/total_rows)*100:.2f}%)\")\n",
    "\n",
    "                current_index += len(current_batch)\n",
    "                \n",
    "                # Clear batch\n",
    "                current_batch = []\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Write evaluation metrics \n",
    "    if withEvaluation:\n",
    "        with open(saveName + \".csv\", 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f, delimiter=';', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "\n",
    "            writer.writerow([\n",
    "                \"Topic\", \"Accuracy\", \"Correct Attempts\", \"Relevant Attempts\",\n",
    "                \"Micro Acc\", \"Micro Prec\", \"Micro Recall\", \"Micro F1\",\n",
    "                \"TP\", \"FP\", \"FN\", \"TN\"\n",
    "            ])\n",
    "\n",
    "            for i, topic_info in enumerate(topics):\n",
    "                sumTP = 0\n",
    "                sumFP = 0\n",
    "                sumFN = 0\n",
    "                sumTN = 0\n",
    "\n",
    "                cat_map = categoryConfusions[i]\n",
    "                for cat_name, conf_map in cat_map.items():\n",
    "                    sumTP += conf_map[\"TP\"]\n",
    "                    sumFP += conf_map[\"FP\"]\n",
    "                    sumFN += conf_map[\"FN\"]\n",
    "                    sumTN += conf_map[\"TN\"]\n",
    "\n",
    "                if numberOfRelevantAttempts[i] > 0:\n",
    "                    accuracy = (numberOfCorrectResults[i] / numberOfRelevantAttempts[i]) * 100.0\n",
    "                else:\n",
    "                    accuracy = -1\n",
    "\n",
    "                micro_accuracy = (sumTP / (sumTP + sumFN)) if (sumTP + sumFN) > 0 else 0.0\n",
    "                micro_precision = (sumTP / (sumTP + sumFP)) if (sumTP + sumFP) > 0 else 0.0\n",
    "                micro_recall = (sumTP / (sumTP + sumFN)) if (sumTP + sumFN) > 0 else 0.0\n",
    "                micro_f1 = 0.0\n",
    "                if micro_precision > 0 and micro_recall > 0:\n",
    "                    micro_f1 = 2.0 * (micro_precision * micro_recall) / (micro_precision + micro_recall)\n",
    "\n",
    "                topic_name = topic_info['topic_input'].value\n",
    "                writer.writerow([\n",
    "                    topic_name,\n",
    "                    f\"{accuracy:.2f}%\",\n",
    "                    numberOfCorrectResults[i],\n",
    "                    numberOfRelevantAttempts[i],\n",
    "                    f\"{micro_accuracy*100:.2f}%\",\n",
    "                    f\"{micro_precision*100:.2f}%\",\n",
    "                    f\"{micro_recall*100:.2f}%\",\n",
    "                    f\"{micro_f1*100:.2f}%\",\n",
    "                    sumTP,\n",
    "                    sumFP,\n",
    "                    sumFN,\n",
    "                    sumTN\n",
    "                ])\n",
    "            #Write elapsed Time\n",
    "            writer.writerow([\"Elapsed Time\", f\"{elapsed_time:.2f} seconds\"])\n",
    "\n",
    "    \n",
    "    print(f\"Classification of '{dataset}.csv' complete. Output written to '{saveName}.csv'.\")\n",
    "    print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "def process_and_write_result(count, row, result, prob, saveName, withEvaluation, \n",
    "                           numberOfCorrectResults, numberOfRelevantAttempts,\n",
    "                           categoryConfusions):\n",
    "    \"\"\"Helper function to process and write a single result\"\"\"\n",
    "    # Update confusion matrices\n",
    "    for tIndex, predCategory in enumerate(result):\n",
    "        if withEvaluation and (tIndex + 1) < len(row):\n",
    "            groundTruth = row[tIndex + 1].strip()\n",
    "            if groundTruth:\n",
    "                for cat_name, conf_map in categoryConfusions[tIndex].items():\n",
    "                    if cat_name == groundTruth and cat_name == predCategory:\n",
    "                        conf_map[\"TP\"] += 1\n",
    "                    elif cat_name != groundTruth and cat_name == predCategory:\n",
    "                        conf_map[\"FP\"] += 1\n",
    "                    elif cat_name == groundTruth and cat_name != predCategory:\n",
    "                        conf_map[\"FN\"] += 1\n",
    "                    else:\n",
    "                        conf_map[\"TN\"] += 1\n",
    "\n",
    "    # Prepare result row\n",
    "    singleResult = [str(count), row[0]]\n",
    "    tmpCount = 0\n",
    "    for ret in result:\n",
    "        tmpCount += 1\n",
    "        if withEvaluation and tmpCount < len(row):\n",
    "            ground_truth = row[tmpCount].strip()\n",
    "            if ground_truth:\n",
    "                numberOfRelevantAttempts[tmpCount - 1] += 1\n",
    "                singleResult.append(ground_truth)\n",
    "                singleResult.append(ret)\n",
    "                if(len(prob)>0):\n",
    "                    singleResult.append(prob.pop(0))\n",
    "                else:\n",
    "                    singleResult.append(\"\")\n",
    "                if ret == ground_truth:\n",
    "                    numberOfCorrectResults[tmpCount - 1] += 1\n",
    "            else:\n",
    "                singleResult.append(\"\")\n",
    "                singleResult.append(\"\")\n",
    "                singleResult.append(\"\")\n",
    "        else:\n",
    "            if not withEvaluation:\n",
    "                singleResult.append(ret)\n",
    "                if(len(prob)>0):\n",
    "                    if not ret==\"\":\n",
    "                        singleResult.append(prob.pop(0))\n",
    "                    else:\n",
    "                        singleResult.append(\"\")\n",
    "                else:\n",
    "                    singleResult.append(\"\")\n",
    "            else:\n",
    "                singleResult.append(\"UNDEFINED\")\n",
    "                singleResult.append(\"UNDEFINED\")\n",
    "\n",
    "    # Write to file\n",
    "    with open(saveName + \".csv\", 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f, delimiter=';', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "        writer.writerow(singleResult)\n",
    "    \n",
    "    \n",
    "def check_prompt_performance_for_topic(\n",
    "    topicId,\n",
    "    dataset,\n",
    "    constrainedOutput=True,\n",
    "    groundTruthCol=None\n",
    "):\n",
    "    csv_file = dataset + \".csv\"\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"No {csv_file} file found.\")\n",
    "        return\n",
    "\n",
    "    found_topic = None\n",
    "    topic_index = None\n",
    "    for i, t in enumerate(topics):\n",
    "        if t.get('id') == topicId:\n",
    "            found_topic = t\n",
    "            topic_index = i\n",
    "            break\n",
    "\n",
    "    if found_topic is None:\n",
    "        print(f\"No topic found with ID={topicId}.\")\n",
    "        return\n",
    "\n",
    "    if groundTruthCol is None:\n",
    "        groundTruthCol = topic_index + 1\n",
    "        \n",
    "    print(topic_index)\n",
    "    print(groundTruthCol)\n",
    "\n",
    "    local_categories = [\n",
    "        cat_input.value\n",
    "        for (cat_input, _, cat_id) in found_topic.get('categories', [])\n",
    "    ]\n",
    "\n",
    "    relevant_attempts = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file, delimiter=';')\n",
    "        rows = list(reader)\n",
    "\n",
    "    for rowIndex, row in enumerate(rows):\n",
    "        if rowIndex == 0:\n",
    "            continue\n",
    "\n",
    "        if len(row) <= groundTruthCol:\n",
    "            continue\n",
    "\n",
    "        text_to_classify = row[0].strip()\n",
    "        groundTruthCategoryName = row[groundTruthCol].strip()\n",
    "        if not groundTruthCategoryName:\n",
    "            continue\n",
    "\n",
    "        prompt_template = found_topic['prompt'].value\n",
    "        prompt_categories_str = \"[\" + \",\".join(f\"'{cat}'\" for cat in local_categories) + \"]\"\n",
    "        prompt = prompt_template.replace('[TOPIC]', found_topic['topic_input'].value)\n",
    "        prompt = prompt.replace('[CATEGORIES]', prompt_categories_str)\n",
    "        prompt = prompt.replace('[TEXT]', text_to_classify)\n",
    "\n",
    "        answer, bestRelProb = getAnswer(prompt, local_categories, constrainedOutput)\n",
    "\n",
    "        relevant_attempts += 1\n",
    "        if answer == groundTruthCategoryName:\n",
    "            correct_predictions += 1\n",
    "            \n",
    "        print(\"Answer:\",answer,\" GT:\",groundTruthCategoryName)\n",
    "\n",
    "    if relevant_attempts > 0:\n",
    "        accuracy = (correct_predictions / relevant_attempts) * 100.0\n",
    "        print(f\"Topic (ID={topicId}) => Accuracy: {accuracy:.2f}%  \"\n",
    "              f\"({correct_predictions} / {relevant_attempts} attempts)\")\n",
    "    else:\n",
    "        print(f\"Topic (ID={topicId}): No relevant attempts (no rows with non-empty groundTruth).\")\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "def getLLMImprovedPromptWithFeedback(old_prompt, old_accuracy, topic_info):\n",
    "    global PromptLLM\n",
    "    topic_name = topic_info['topic_input'].value\n",
    "    category_list = [cat_input.value for (cat_input, _, _cat_id) in topic_info['categories']]\n",
    "    category_str = \", \".join(category_list) if category_list else \"No categories defined\"\n",
    "\n",
    "    system_content = (\n",
    "        f\"You are an advanced prompt engineer.\\n\"\n",
    "        f\"The classification topic is '{topic_name}'.\\n\"\n",
    "        f\"The available categories for this topic are: {category_str}\\n\"\n",
    "        \"Rewrite the user's prompt to achieve higher accuracy on classification tasks.\\n\"\n",
    "        \"You MUST keep the placeholder [TEXT].\\n\"\n",
    "        \"IMPORTANT: Output ONLY the final prompt, wrapped in triple backticks.\\n\"\n",
    "        \"No commentary, bullet points, or explanations.\\n\"\n",
    "        \"The new prompt should be in English.\\n\"\n",
    "    )\n",
    "\n",
    "    user_content = (\n",
    "        f\"Previously, the prompt achieved an accuracy of {old_accuracy:.2f}%. \\n\"\n",
    "        \"Here is the old prompt:\\n\\n\"\n",
    "        f\"{old_prompt}\\n\\n\"\n",
    "        \"Please rewrite/improve this prompt. Keep [TEXT]. \"\n",
    "        \"Wrap your entire revised prompt in triple backticks, with no extra lines.\"\n",
    "    )\n",
    "\n",
    "    if promptModelType in (\"OpenAI\", \"DeepInfra\"):\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=promptModel,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_content},\n",
    "                    {\"role\": \"user\", \"content\": user_content}\n",
    "                ],\n",
    "                max_tokens=250,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            improved_prompt = completion.choices[0].message.content.strip()\n",
    "\n",
    "            match = re.search(r\"```(.*?)```\", improved_prompt, flags=re.DOTALL)\n",
    "            if match:\n",
    "                improved_prompt = match.group(1).strip()\n",
    "            else:\n",
    "                print(\"Warning: The LLM did not provide triple backticks. Using full text.\")\n",
    "\n",
    "            print(\"Improved Prompt:\", improved_prompt)  # Debug\n",
    "\n",
    "            if not improved_prompt or \"[TEXT]\" not in improved_prompt:\n",
    "                print(\"Warning: The improved prompt is empty or lacks [TEXT]. Reverting to old prompt.\")\n",
    "                return old_prompt\n",
    "\n",
    "            return improved_prompt\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling OpenAI/DeepInfra: {e}\")\n",
    "            return old_prompt\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            base_instruction = system_content\n",
    "            improvement_request = (\n",
    "                f\"{base_instruction}\\n\\n\"\n",
    "                f\"Original prompt:\\n{old_prompt}\\n\"\n",
    "            )\n",
    "\n",
    "            \n",
    "            if promptInferenceType==\"transformers\":\n",
    "                inputs = promptModelTokenizer(improvement_request, return_tensors=\"pt\").to(\"cuda\")\n",
    "                period_id = promptModelTokenizer.encode(\".\", add_special_tokens=False)[-1]\n",
    "                question_id = tokenizer.encode(\"?\", add_special_tokens=False)[-1]\n",
    "                exclamation_id = tokenizer.encode(\"!\", add_special_tokens=False)[-1]\n",
    "                stopping_criteria = StoppingCriteriaList([\n",
    "                    StopOnTokens([period_id, question_id, exclamation_id])\n",
    "                ])\n",
    "                outputs = PromptLLM.generate(inputs.input_ids, temperature=0.01, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95, stopping_criteria=stopping_criteria)\n",
    "                new_prompt=tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "            elif promptInferenceType==\"guidance\":\n",
    "                script = promptModelGuidance + f\" {improvement_request}\" + gen(max_tokens=250, name='improvedPrompt')\n",
    "                new_prompt = script[\"improvedPrompt\"]\n",
    "\n",
    "            if not new_prompt or \"[TEXT]\" not in new_prompt:\n",
    "                print(\"Warning: The improved prompt is empty or lacks [TEXT]. Reverting to old prompt.\")\n",
    "                return old_prompt\n",
    "\n",
    "            return new_prompt\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling local approach: {e}\")\n",
    "            return old_prompt\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "def evaluate_prompt_accuracy(topic_info, prompt, dataset, constrainedOutput, groundTruthCol):\n",
    "    csv_file = dataset + \".csv\"\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"No {csv_file} file found.\")\n",
    "        return 0.0\n",
    "\n",
    "    local_categories = [cat_input.value for (cat_input, _, _) in topic_info.get('categories', [])]\n",
    "    relevant_attempts = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file, delimiter=';')\n",
    "        rows = list(reader)\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "        if i == 0: \n",
    "            continue\n",
    "        if len(row) <= groundTruthCol:\n",
    "            continue\n",
    "\n",
    "        text_to_classify = row[0].strip()\n",
    "        groundTruthCategoryName = row[groundTruthCol].strip()\n",
    "        if not groundTruthCategoryName:\n",
    "            continue\n",
    "\n",
    "        prompt_categories_str = \"[\" + \",\".join(f\"'{cat}'\" for cat in local_categories) + \"]\"\n",
    "        final_prompt = prompt.replace('[TOPIC]', topic_info['topic_input'].value)\n",
    "        final_prompt = final_prompt.replace('[CATEGORIES]', prompt_categories_str)\n",
    "        final_prompt = final_prompt.replace('[TEXT]', text_to_classify)\n",
    "\n",
    "        answer, bestRelProb = getAnswer(final_prompt, local_categories, constrainedOutput)\n",
    "        relevant_attempts += 1\n",
    "        if answer == groundTruthCategoryName:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    if relevant_attempts > 0:\n",
    "        return (correct_predictions / relevant_attempts) * 100.0\n",
    "    return 0.0    \n",
    "\n",
    "        \n",
    "def improve_prompt(topicId, dataset, constrainedOutput=True, groundTruthCol=None, num_iterations=10):\n",
    "    found_topic = next((t for t in topics if t.get('id') == topicId), None)\n",
    "    if not found_topic:\n",
    "        print(f\"No topic found with ID {topicId}.\")\n",
    "        return\n",
    "    \n",
    "    topic_index = topics.index(found_topic)\n",
    "    if groundTruthCol is None:\n",
    "        groundTruthCol = (topic_index * 2) + 1\n",
    "\n",
    "    old_prompt = found_topic['prompt'].value\n",
    "    old_accuracy = evaluate_prompt_accuracy(found_topic, old_prompt, dataset, constrainedOutput, groundTruthCol)\n",
    "\n",
    "    best_prompt = old_prompt\n",
    "    best_accuracy = old_accuracy\n",
    "\n",
    "    print(\"========================================\")\n",
    "    print(f\"Starting iterative prompt improvement for topic '{found_topic['id']}'\")\n",
    "    print(f\"Baseline accuracy: {best_accuracy:.2f}%\")\n",
    "    print(\"========================================\")\n",
    "\n",
    "    for iteration in range(1, num_iterations + 1):\n",
    "        new_prompt = getLLMImprovedPromptWithFeedback(best_prompt, best_accuracy, found_topic)\n",
    "        if \"[TEXT]\" not in new_prompt:\n",
    "            print(\"Warning: The improved prompt lost [TEXT]. Skipping iteration.\")\n",
    "            continue\n",
    "\n",
    "        new_accuracy = evaluate_prompt_accuracy(found_topic, new_prompt, dataset, constrainedOutput, groundTruthCol)\n",
    "        diff = new_accuracy - best_accuracy\n",
    "\n",
    "        print(f\"Iteration {iteration}:\")\n",
    "        print(f\"New prompt accuracy: {new_accuracy:.2f}% (was {best_accuracy:.2f}%)\")\n",
    "\n",
    "        if diff > 0.001:\n",
    "            print(f\"Improvement found (+{diff:.2f}%). Updating best prompt.\")\n",
    "            best_prompt = new_prompt\n",
    "            best_accuracy = new_accuracy\n",
    "        else:\n",
    "            print(\"No improvement. Keeping current best prompt.\")\n",
    "        print(\"----------------------------------------\")\n",
    "\n",
    "    print(\"========================================\")\n",
    "    print(f\"Final best accuracy: {best_accuracy:.2f}%\")\n",
    "    print(\"Best prompt:\\n\", best_prompt)\n",
    "    print(\"========================================\\n\")\n",
    "\n",
    "    if best_accuracy > old_accuracy:\n",
    "        found_topic['best_prompt_found'] = best_prompt\n",
    "        found_topic['best_prompt_accuracy'] = best_accuracy\n",
    "    else:\n",
    "        found_topic['best_prompt_found'] = None\n",
    "        found_topic['best_prompt_accuracy'] = None\n",
    "        \n",
    "        \n",
    "def setPrompt(topicId, newPrompt):\n",
    "    for topic in topics:\n",
    "        if topic.get('id') == topicId:\n",
    "            if 'prompt' in topic and hasattr(topic['prompt'], 'value'):\n",
    "                topic['prompt'].value = newPrompt\n",
    "            else:\n",
    "                topic['prompt'] = MockText(newPrompt)\n",
    "            print(f\"Prompt for topic ID {topicId} updated.\")\n",
    "            return\n",
    "\n",
    "    print(f\"Topic with ID {topicId} not found.\")\n",
    "\n",
    "def removeAllTopics():\n",
    "    global topics, topic_id_counter, previous_results, selectOptions\n",
    "    \n",
    "    topics.clear()            \n",
    "    topic_id_counter = 0         \n",
    "    previous_results.clear()    \n",
    "    if 'selectOptions' in globals():\n",
    "        selectOptions.clear()   \n",
    "    \n",
    "    print(\"All topics have been removed, counters reset, and related data cleared.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "                \n",
    "        \n",
    "class MockText:\n",
    "    def __init__(self, value: str):\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### UI\n",
    "def check_temporary_prompt_performance(topic_info, custom_prompt):\n",
    "    global classify_CSV_box\n",
    "    global with_Evaluation_Checkbox\n",
    "    global constrainedOutputCheckbox\n",
    "    \"\"\"\n",
    "    Evaluate how well a *custom* prompt performs (without overwriting topic_info['prompt']).\n",
    "    Returns the accuracy as a float (0..100).\n",
    "    Skips rows with empty groundTruth.\n",
    "    \"\"\"\n",
    "    topic_index = topics.index(topic_info)\n",
    "    dataset_name = classify_CSV_box.children[0].value\n",
    "    withEvaluation = with_Evaluation_Checkbox.value\n",
    "    constrainedOutput = constrainedOutputCheckbox.value\n",
    "\n",
    "    csv_file = dataset_name + \".csv\"\n",
    "    if not os.path.exists(csv_file):\n",
    "        return 0.0 \n",
    "\n",
    "    local_categories = [cat_input.value for (cat_input, _, cat_id) in topic_info['categories']]\n",
    "\n",
    "    relevant_attempts = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "        rows = list(csv.reader(file, delimiter=';'))\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "        if i == 0:\n",
    "            continue  # skip header\n",
    "\n",
    "        if len(row) <= topic_index:\n",
    "            continue\n",
    "\n",
    "        text_to_classify = row[0]\n",
    "        if withEvaluation:\n",
    "            groundTruthCategoryName = row[topic_index + 1].strip() if len(row) > topic_index else \"\"\n",
    "        else:\n",
    "            groundTruthCategoryName = \"\"\n",
    "\n",
    "        if not groundTruthCategoryName:\n",
    "            # Condition not met or empty groundTruth => skip\n",
    "            continue\n",
    "\n",
    "        # Build categories string for the custom prompt\n",
    "        prompt_categories_str = \"[\" + \",\".join(f\"'{cat}'\" for cat in local_categories) + \"]\"\n",
    "\n",
    "        # Replace placeholders in the custom prompt\n",
    "        tmp_prompt = custom_prompt.replace('[TOPIC]', topic_info['topic_input'].value)\n",
    "        tmp_prompt = tmp_prompt.replace('[CATEGORIES]', prompt_categories_str)\n",
    "        tmp_prompt = tmp_prompt.replace('[TEXT]', text_to_classify)\n",
    "\n",
    "        # Classify\n",
    "        answer, bestRelProb = getAnswer(tmp_prompt, local_categories, constrainedOutput)\n",
    "        \n",
    "\n",
    "        relevant_attempts += 1\n",
    "        if answer == groundTruthCategoryName:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    if relevant_attempts > 0:\n",
    "        return (correct_predictions / relevant_attempts) * 100.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def openInterface():\n",
    "    global classify_CSV_box\n",
    "    global with_Evaluation_Checkbox\n",
    "    global constrainedOutputCheckbox\n",
    "    global topics\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output\n",
    "    import json\n",
    "    import os\n",
    "    def on_value_change(change):\n",
    "        if change['new']:  # Check if the new value is True (checkbox checked)\n",
    "            freeText_input.layout.visibility = 'hidden'\n",
    "            classify_CSV_box.layout.visibility = 'visible'\n",
    "        else:\n",
    "            freeText_input.layout.visibility = 'visible'\n",
    "            classify_CSV_box.layout.visibility = 'hidden'\n",
    "    def add_topic_ui(b=None, topic_data=None):\n",
    "        global topic_id_counter\n",
    "        topic_id_counter += 1\n",
    "        topic_id = number_to_letters(topic_id_counter, uppercase=True)\n",
    "\n",
    "        topic_input = widgets.Text(\n",
    "            value=topic_data['topic_input'] if topic_data else '',\n",
    "            description=f'Topic {topic_id}:',\n",
    "            disabled=False\n",
    "        )\n",
    "        condition_input = widgets.Text(\n",
    "            value=topic_data['condition'] if topic_data and 'condition' in topic_data else '',\n",
    "            description='Condition:',\n",
    "            disabled=False\n",
    "        )\n",
    "\n",
    "        add_category_button = widgets.Button(description=\"Add Category\")\n",
    "        remove_topic_button = widgets.Button(description=\"Remove Topic\")\n",
    "        categories_container = widgets.VBox()\n",
    "\n",
    "        # Prompt\n",
    "        tmpPrompt = topic_data['prompt'] if topic_data and 'prompt' in topic_data else (\n",
    "            \"INSTRUCTION: You are a helpful classifier. You select the correct of the possible categories \"\n",
    "            \"for classifying a piece of text. The topic of the classification is '[TOPIC]'. \"\n",
    "            \"The allowed categories are '[CATEGORIES]'. QUESTION: The text to be classified is '[TEXT]'. \"\n",
    "            \"ANSWER: The correct category for this text is '\"\n",
    "        )\n",
    "        prompt_input = widgets.Text(value=tmpPrompt, description='Prompt:', disabled=False)\n",
    "\n",
    "        # Performance label\n",
    "        performance_label = widgets.Label(value=\"\")\n",
    "\n",
    "        # Buttons: Check Prompt Performance, iteration input, iteration improvement, and \"Replace Prompt\"\n",
    "        checkPrompt_button = widgets.Button(description=\"Check Prompt Performance\")\n",
    "        checkPrompt_button.layout.display = \"none\"\n",
    "\n",
    "        num_iterations_input = widgets.IntText(\n",
    "            value=1,\n",
    "            description='Iterations:',\n",
    "            layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "        num_iterations_input.layout.display = \"none\"\n",
    "\n",
    "        iteratePromptImprovements_button = widgets.Button(description=\"Iterate Prompt Improvement\")\n",
    "        iteratePromptImprovements_button.layout.display = \"none\"\n",
    "\n",
    "        replacePrompt_button = widgets.Button(description=\"Replace Prompt (New Accuracy: ?)\")\n",
    "        replacePrompt_button.layout.display = \"none\"  # hidden unless a better prompt is found\n",
    "\n",
    "\n",
    "\n",
    "        # Row 1: prompt, performance label, + add/remove category\n",
    "        prompt_row = widgets.HBox([\n",
    "            prompt_input,\n",
    "            performance_label,\n",
    "            add_category_button,\n",
    "            remove_topic_button\n",
    "        ])\n",
    "\n",
    "        # Row 2: performance/iteration controls + the hidden \"Replace Prompt\" button\n",
    "        performance_row = widgets.HBox([\n",
    "            checkPrompt_button,\n",
    "            num_iterations_input,\n",
    "            iteratePromptImprovements_button,\n",
    "            replacePrompt_button\n",
    "        ])\n",
    "\n",
    "        topic_box = widgets.VBox([\n",
    "            widgets.HBox([topic_input, condition_input]),\n",
    "            prompt_row,\n",
    "            performance_row,\n",
    "            categories_container\n",
    "        ])\n",
    "\n",
    "        # Construct topic_info now that we have all widgets\n",
    "        topic_info = {\n",
    "            'id': topic_id,\n",
    "            'topic_input': topic_input,\n",
    "            'condition': condition_input,\n",
    "            'categories': [],\n",
    "            'prompt': prompt_input,\n",
    "            'categories_container': categories_container,\n",
    "            'topic_box': topic_box,\n",
    "\n",
    "            'performance_label': performance_label,\n",
    "            'checkPrompt_button': checkPrompt_button,\n",
    "            'num_iterations_input': num_iterations_input,\n",
    "            'iteratePromptImprovements_button': iteratePromptImprovements_button,\n",
    "            'replacePrompt_button': replacePrompt_button,\n",
    "\n",
    "            # We'll store the best prompt found after iteration in here\n",
    "            'best_prompt_found': None,\n",
    "            'best_prompt_accuracy': None\n",
    "        }\n",
    "\n",
    "        # Callbacks\n",
    "        checkPrompt_button.on_click(lambda btn: check_prompt_performance_for_topic_ui(topic_info))\n",
    "\n",
    "        def do_iterative_prompt_improvement_callback(btn):\n",
    "            replacePrompt_button.layout.display = \"none\"\n",
    "            iterations = num_iterations_input.value\n",
    "            iterative_prompt_improvement_ui(topic_info, iterations)\n",
    "\n",
    "        iteratePromptImprovements_button.on_click(do_iterative_prompt_improvement_callback)\n",
    "\n",
    "        # When \"Replace Prompt\" is clicked, update the prompt input with the new best prompt\n",
    "        def replace_prompt_callback(btn):\n",
    "            if topic_info['best_prompt_found'] is not None:\n",
    "                topic_info['prompt'].value = topic_info['best_prompt_found']\n",
    "                # Optionally re-check performance immediately\n",
    "                check_prompt_performance_for_topic_ui(topic_info)\n",
    "            # Hide the button again or keep it, your choice:\n",
    "            topic_info['replacePrompt_button'].layout.display = \"none\"\n",
    "\n",
    "        replacePrompt_button.on_click(replace_prompt_callback)\n",
    "\n",
    "        # Category add/remove callbacks\n",
    "        add_category_button.on_click(lambda btn: add_category_ui(topic_info))\n",
    "        remove_topic_button.on_click(lambda btn: remove_topic_ui(topic_info))\n",
    "\n",
    "        topics.append(topic_info)\n",
    "        update_topics_container()\n",
    "\n",
    "        # Load existing categories if present\n",
    "        if topic_data and 'categories' in topic_data:\n",
    "            for category_data in topic_data['categories']:\n",
    "                add_category_ui(topic_info, category_data)\n",
    "    def on_evaluation_toggle(change):\n",
    "        new_value = change['new']  # True if checked, False if unchecked\n",
    "        for topic_info in topics:\n",
    "            # Check Prompt Performance\n",
    "            if 'checkPrompt_button' in topic_info:\n",
    "                topic_info['checkPrompt_button'].layout.display = \"inline-block\" if new_value else \"none\"\n",
    "\n",
    "            # show/hide iteration input & button\n",
    "            if 'num_iterations_input' in topic_info:\n",
    "                topic_info['num_iterations_input'].layout.display = \"inline-block\" if new_value else \"none\"\n",
    "            if 'iteratePromptImprovements_button' in topic_info:\n",
    "                topic_info['iteratePromptImprovements_button'].layout.display = \"inline-block\" if new_value else \"none\"\n",
    "\n",
    "            # Possibly clear performance label if disabling\n",
    "            if not new_value and 'performance_label' in topic_info:\n",
    "                topic_info['performance_label'].value = \"\"\n",
    "                all_num_iterations_input.layout.display = \"none\"\n",
    "                improveAllPrompts_button.layout.display = \"none\"\n",
    "            else:\n",
    "                all_num_iterations_input.layout.display = \"inline-block\"\n",
    "                improveAllPrompts_button.layout.display = \"inline-block\"\n",
    "    # Function to add new category input fields to a specific topic\n",
    "    def add_category_ui(topic_info, category_data=None):\n",
    "        topic_info.setdefault('category_counter', 0)  # Initialize the category counter if not present\n",
    "        topic_info['category_counter'] += 1\n",
    "        category_id = number_to_letters(topic_info['category_counter'], uppercase=False)  # Generate category ID as lowercase letters\n",
    "\n",
    "        new_category_input = widgets.Text(\n",
    "            value=category_data['value'] if category_data else '',\n",
    "            description=f'Category {category_id}:',\n",
    "            disabled=False\n",
    "        )\n",
    "        remove_button = widgets.Button(description=\"Remove Category\")\n",
    "        category_box = widgets.HBox([new_category_input, remove_button])\n",
    "\n",
    "        remove_button.on_click(lambda btn: remove_category_ui(topic_info, category_box))\n",
    "\n",
    "        topic_info['categories'].append((new_category_input, category_box, category_id))\n",
    "        update_categories_container(topic_info)\n",
    "\n",
    "    # Function to remove a specific category from a specific topic\n",
    "    def remove_category_ui(topic_info, category_box):\n",
    "        topic_info['categories'] = [(input_field, box, cat_id) \n",
    "                                    for (input_field, box, cat_id) in topic_info['categories'] \n",
    "                                    if box != category_box]\n",
    "        update_categories_container(topic_info)\n",
    "\n",
    "    # Function to update the categories container for a specific topic\n",
    "    def update_categories_container(topic_info):\n",
    "        topic_info['categories_container'].children = [box for _, box, _ in topic_info['categories']]\n",
    "        if interface:\n",
    "            update_topics_container()\n",
    "\n",
    "    # Function to remove a specific topic\n",
    "    def remove_topic_ui(topic_info):\n",
    "        topics.remove(topic_info)\n",
    "        update_topics_container()\n",
    "\n",
    "    # Function to update the topics container\n",
    "    def update_topics_container():\n",
    "        topics_container.children = [td['topic_box'] for td in topics]         \n",
    "\n",
    "\n",
    "    # Save topics to a local file\n",
    "    def save_topics_ui(b):\n",
    "        saveFileName = save_container.children[0].value + \".json\"\n",
    "        save_topics(saveFileName)\n",
    "\n",
    "\n",
    "    # Load topics from a local file\n",
    "    def load_topics_ui(b):\n",
    "        global topic_id_counter\n",
    "        global topics\n",
    "        topic_id_counter=0\n",
    "        loadFileName = load_container.children[0].value + \".json\"\n",
    "        if os.path.exists(loadFileName):\n",
    "            with open(loadFileName, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            topics = []\n",
    "            topics_container.children = []\n",
    "            for topic_data in data:\n",
    "                add_topic_ui(topic_data=topic_data)\n",
    "            if interface:\n",
    "                print(f\"Loaded topics from {loadFileName}\")\n",
    "        else:\n",
    "            if interface:\n",
    "                print(f\"No {loadFileName} file found\")\n",
    "                \n",
    "    def check_prompt_performance_for_topic_ui(\n",
    "        topic_info,\n",
    "        constrainedOutput=True,\n",
    "        groundTruthCol=None\n",
    "    ):\n",
    "        topic_info['performance_label'].value = f\" \"\n",
    "        # 1) Figure out which topic index this is among 'topics'\n",
    "        topic_index = topics.index(topic_info)\n",
    "        topic=topics[topic_index]\n",
    "        topicId=topic.get('id')\n",
    "        check_prompt_performance_for_topic(topicId,classify_CSV_box.children[0].value)\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "    def do_Classification_Button_Function():\n",
    "        global selectOptions\n",
    "        # Record the start time\n",
    "        start_time = time.time()\n",
    "        numberOfCorrectResults = []\n",
    "        if groupClassificationCheckbox.value: #Group Classification\n",
    "            classify_table(classify_CSV_box.children[0].value, with_Evaluation_Checkbox.value, constrainedOutputCheckbox.value, BATCH_SIZE=100)\n",
    "            #groupClassification(classify_CSV_box.children[0].value,with_Evaluation_Checkbox.value,numberOfCorrectResults,constrainedOutputCheckbox.value)\n",
    "        else:            \n",
    "            classify(freeText_container.children[0].value, True, constrainedOutputCheckbox.value)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        saveName=classify_CSV_box.children[0].value+\"_(result)\"\n",
    "        \n",
    "        ##Save for GroupClassification\n",
    "        if groupClassificationCheckbox.value:\n",
    "            with open(saveName+\"(single).csv\",'a',newline='',encoding='utf-8') as f:\n",
    "                singleResult=[]\n",
    "                singleResult.append(str(elapsed_time)+\"seconds\")\n",
    "                writer = csv.writer(f , delimiter=';', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "                writer.writerow(singleResult)\n",
    "                singleResult=[]\n",
    "                singleResult.append(\"Model:\")\n",
    "                singleResult.append(model)\n",
    "                writer = csv.writer(f , delimiter=';', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "                writer.writerow(singleResult)\n",
    "                singleResult=[]\n",
    "                singleResult.append(\"ModelType:\")\n",
    "                singleResult.append(modelType)\n",
    "                writer = csv.writer(f , delimiter=';', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "                writer.writerow(singleResult)\n",
    "                for l in range(len(selectOptions)):\n",
    "                    singleResult=[\"Prompt:\"]\n",
    "                    singleResult.append(topics[l]['prompt'].value)\n",
    "                    singleResult.append(\"Topic:\")\n",
    "                    singleResult.append(topics[l]['topic_input'].value)\n",
    "                    singleResult.append(\"Categories:\")\n",
    "                    singleResult.append(selectOptions[l])\n",
    "                    writer = csv.writer(f , delimiter=';', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "                    writer.writerow(singleResult)\n",
    "\n",
    "                singleResult=[\"Time:\"]\n",
    "                singleResult.append(str(elapsed_time)+\"seconds\")\n",
    "                writer = csv.writer(f , delimiter=';', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "                writer.writerow(singleResult)\n",
    "\n",
    "        print(f\"This classification took {elapsed_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def getLLMImprovedPromptWithFeedback_ui(old_prompt, old_accuracy, topic_info):\n",
    "        \"\"\"\n",
    "        Similar to getLLMImprovedPrompt, but also provides the last iteration's accuracy \n",
    "        to the LLM so it can attempt to improve further. Additionally, \n",
    "        we add the topic name AND the categories in the system content for context.\n",
    "        \"\"\"\n",
    "        topic_name = topic_info['topic_input'].value  # e.g., \"Breast Cancer Family History\"\n",
    "        # Build a list of category names for this topic\n",
    "        category_list = [cat_input.value for (cat_input, _, cat_id) in topic_info['categories']]\n",
    "        category_str = \", \".join(category_list) if category_list else \"No categories defined\"\n",
    "\n",
    "        # We'll reuse your existing model selection logic\n",
    "        if promptModelType in (\"OpenAI\", \"DeepInfra\"):\n",
    "            try:\n",
    "                # Include topic name and categories in the system_content\n",
    "                system_content = (\n",
    "                    f\"You are an advanced prompt engineer.\\n\"\n",
    "                    f\"The text to be classified is a german mammography report\\n\"\n",
    "                    f\"The classification topic is '{topic_name}'.\\n\"\n",
    "                    f\"The available categories for this topic are: {category_str}\\n\"\n",
    "                    \"Rewrite the user's prompt to achieve higher accuracy on classification tasks.\\n\"\n",
    "                    \"You MUST keep the placeholder [TEXT].\\n\"\n",
    "                    \"IMPORTANT: Output ONLY the final prompt, wrapped in triple backticks.\\n\"\n",
    "                    \"No commentary, bullet points, or explanations.\"\n",
    "                    \"The new prompt should be in english\"\n",
    "                )\n",
    "\n",
    "                user_content = (\n",
    "                    f\"Previously, the prompt achieved an accuracy of {old_accuracy:.2f}%. \\n\"\n",
    "                    \"Here is the old prompt:\\n\\n\"\n",
    "                    f\"{old_prompt}\\n\\n\"\n",
    "                    \"Please rewrite/improve this prompt. Keep [TEXT]. \"\n",
    "                    \"Wrap your entire revised prompt in triple backticks, with no extra lines.\"\n",
    "                )\n",
    "\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=promptModel,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_content},\n",
    "                        {\"role\": \"user\", \"content\": user_content}\n",
    "                    ],\n",
    "                    max_tokens=250,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                improved_prompt = completion.choices[0].message.content.strip()\n",
    "\n",
    "                # Now parse out whatever is between triple backticks.\n",
    "                match = re.search(r\"```(.*?)```\", improved_prompt, flags=re.DOTALL)\n",
    "                if match:\n",
    "                    # Extract the text inside ```...```\n",
    "                    improved_prompt = match.group(1).strip()\n",
    "                else:\n",
    "                    print(\"Warning: The LLM did not provide triple backticks. Using full text.\")\n",
    "\n",
    "                print(\"Improved Prompt:\", improved_prompt)  # Debug\n",
    "\n",
    "                # Basic check\n",
    "                if not improved_prompt or \"[TEXT]\" not in improved_prompt:\n",
    "                    print(\"Warning: The improved prompt is empty or lacks [TEXT]. Reverting to old prompt.\")\n",
    "                    return old_prompt\n",
    "\n",
    "                return improved_prompt\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error calling OpenAI/DeepInfra: {e}\")\n",
    "                return old_prompt\n",
    "\n",
    "        else:\n",
    "            # Local approach\n",
    "            try:\n",
    "                base_instruction = (\n",
    "                    f\"The previous iteration got {old_accuracy:.2f}% accuracy. \\n\"\n",
    "                    f\"The classification topic is '{topic_name}'.\\n\"\n",
    "                    f\"The available categories are: {category_str}\\n\"\n",
    "                    \"Please refine the prompt below to achieve higher accuracy, keeping [TEXT].\"\n",
    "                )\n",
    "                improvement_request = f\"{base_instruction}\\n\\nOriginal prompt:\\n{old_prompt}\\n\"\n",
    "\n",
    "                if constrainedOutputCheckbox.value:\n",
    "                    script = ModelGuidance + f\" {improvement_request}\" + select(\n",
    "                        options=[\"ImprovementA\", \"ImprovementB\", \"ImprovementC\"],\n",
    "                        name='improvedPrompt'\n",
    "                    )\n",
    "                    new_prompt = script[\"improvedPrompt\"]\n",
    "                else:\n",
    "                    script = ModelGuidance + f\" {improvement_request}\" + gen(max_tokens=250, name='improvedPrompt')\n",
    "                    new_prompt = script[\"improvedPrompt\"]\n",
    "\n",
    "                if not new_prompt or \"[TEXT]\" not in new_prompt:\n",
    "                    print(\"Warning: The improved prompt is empty or lacks [TEXT]. Reverting to old prompt.\")\n",
    "                    return old_prompt\n",
    "\n",
    "                return new_prompt\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error calling local approach: {e}\")\n",
    "                return old_prompt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def iterative_prompt_improvement_ui(topic_info, num_iterations=10):\n",
    "        old_prompt = topic_info['prompt'].value\n",
    "        old_accuracy = get_current_accuracy(topic_info)\n",
    "        if old_accuracy <= 0:\n",
    "            old_accuracy = check_temporary_prompt_performance(topic_info, old_prompt)\n",
    "\n",
    "        best_prompt = old_prompt\n",
    "        best_accuracy = old_accuracy\n",
    "\n",
    "        print(\"========================================\")\n",
    "        print(f\"Starting iterative prompt improvement for topic '{topic_info['id']}'\")\n",
    "        print(f\"Baseline accuracy: {best_accuracy:.2f}%\")\n",
    "        print(\"========================================\")\n",
    "        \n",
    "        if best_accuracy <100:\n",
    "            for iteration in range(1, num_iterations + 1):\n",
    "                new_prompt = getLLMImprovedPromptWithFeedback_ui(best_prompt, best_accuracy, topic_info)\n",
    "                if \"[TEXT]\" not in new_prompt:\n",
    "                    print(\"Warning: The improved prompt lost [TEXT]. Skipping iteration.\")\n",
    "                    continue\n",
    "\n",
    "                new_accuracy = check_temporary_prompt_performance(topic_info, new_prompt)\n",
    "                diff = new_accuracy - best_accuracy\n",
    "\n",
    "                print(f\"New prompt accuracy: {new_accuracy:.2f}% (was {best_accuracy:.2f}%)\")\n",
    "\n",
    "                if diff > 0.001:\n",
    "                    print(f\"Improvement found (+{diff:.2f}%). Updating best prompt.\")\n",
    "                    best_prompt = new_prompt\n",
    "                    best_accuracy = new_accuracy\n",
    "                else:\n",
    "                    print(f\"No improvement. Keeping old prompt.\")\n",
    "                print(\"----------------------------------------\")\n",
    "\n",
    "            print(\"========================================\")\n",
    "            print(f\"Final best accuracy: {best_accuracy:.2f}%\")\n",
    "            print(\"Best prompt:\\n\", best_prompt)\n",
    "            print(\"========================================\\n\")\n",
    "\n",
    "            # If the best accuracy is better than the original, store it in topic_info\n",
    "            if best_accuracy > old_accuracy:\n",
    "                topic_info['best_prompt_found'] = best_prompt\n",
    "                topic_info['best_prompt_accuracy'] = best_accuracy\n",
    "\n",
    "                # Update the label on the replacePrompt_button\n",
    "                label_text = f\"Replace Prompt (New Accuracy: {best_accuracy:.2f}%)\"\n",
    "                topic_info['replacePrompt_button'].description = label_text\n",
    "                # Show the button\n",
    "                topic_info['replacePrompt_button'].layout.display = \"inline-block\"\n",
    "            else:\n",
    "                # Otherwise, hide the button (or keep hidden)\n",
    "                topic_info['replacePrompt_button'].layout.display = \"none\"\n",
    "                topic_info['best_prompt_found'] = None\n",
    "                topic_info['best_prompt_accuracy'] = None\n",
    "\n",
    "\n",
    "    def do_iterative_prompt_improvement_for_all_ui(btn):\n",
    "            iterations = all_num_iterations_input.value\n",
    "            print(f\"Starting iterative improvement of all prompts for {iterations} iterations each.\")\n",
    "\n",
    "            # For each topic, call iterative_prompt_improvement\n",
    "            for i, topic_info in enumerate(topics):\n",
    "                print(f\"\\n>>> Improving topic #{i+1} (ID {topic_info['id']})\")\n",
    "                iterative_prompt_improvement_ui(topic_info, iterations)\n",
    "                \n",
    "    groupClassificationCheckbox=widgets.Checkbox(value=False, description='Group Classification', disabled=False )\n",
    "    display(groupClassificationCheckbox)\n",
    "\n",
    "    groupClassificationCheckbox.observe(on_value_change, names='value')\n",
    "\n",
    "    #ConstrainedOutput Checkbox\n",
    "    constrainedOutputCheckbox=widgets.Checkbox(\n",
    "        value=True,\n",
    "        description='Constrained Output',\n",
    "        disabled=False\n",
    "    )\n",
    "    display(constrainedOutputCheckbox)\n",
    "\n",
    "\n",
    "\n",
    "    #FreeText Container\n",
    "    freeText_container = widgets.VBox()\n",
    "    display(freeText_container)\n",
    "\n",
    "    freeText_input = widgets.Text(\n",
    "        value=\"\",\n",
    "        description='FreeText:',\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "    freeText_container.children = [freeText_input]\n",
    "\n",
    "\n",
    "\n",
    "    #Classify CSV Container\n",
    "    classify_CSV_input = widgets.Text(\n",
    "            value=\"data\",\n",
    "            description='CSV File:',\n",
    "            disabled=False\n",
    "        )\n",
    "\n",
    "    with_Evaluation_Checkbox=widgets.Checkbox(\n",
    "        value=False,\n",
    "        description='Evaluation',\n",
    "        disabled=False\n",
    "    )\n",
    "    with_Evaluation_Checkbox.observe(on_evaluation_toggle, names='value')\n",
    "    \n",
    "\n",
    "    classify_CSV_box = widgets.HBox([classify_CSV_input,with_Evaluation_Checkbox])\n",
    "    classify_CSV_box.layout.visibility = \"hidden\"\n",
    "\n",
    "    display(classify_CSV_box)\n",
    "    \n",
    "    \n",
    "    \n",
    "    all_num_iterations_input = widgets.IntText(\n",
    "        value=1,\n",
    "        description='All Iterations:',\n",
    "        layout=widgets.Layout(width='150px')\n",
    "    )\n",
    "    # Hide or show only when with_Evaluation is True, at your preference:\n",
    "    all_num_iterations_input.layout.display = \"none\"\n",
    "    \n",
    "    # The button to iteratively improve all prompts\n",
    "    improveAllPrompts_button = widgets.Button(description=\"Iteratively Improve all Prompts\")\n",
    "    improveAllPrompts_button.layout.display = \"none\"\n",
    "\n",
    "    display(widgets.HBox([all_num_iterations_input, improveAllPrompts_button]))\n",
    "    improveAllPrompts_button.on_click(do_iterative_prompt_improvement_for_all_ui)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Container to hold all topics\n",
    "    topics_container = widgets.VBox()\n",
    "    display(topics_container)\n",
    "\n",
    "    # List to keep track of all topics and their categories\n",
    "\n",
    "\n",
    "    # Function to add a new topic with its own category container\n",
    "\n",
    "\n",
    "    plus_topic_button = widgets.Button(description=\"Add Topic\")\n",
    "    plus_topic_button.on_click(add_topic_ui)\n",
    "    display(plus_topic_button)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    save_container = widgets.HBox()\n",
    "    display(save_container)\n",
    "\n",
    "\n",
    "\n",
    "    saveFileText_input = widgets.Text(\n",
    "        value=\"topics\",\n",
    "        description='File Name:',\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "\n",
    "    save_button = widgets.Button(description=\"Save Topics\")\n",
    "    save_button.on_click(save_topics_ui)\n",
    "    save_container.children = [saveFileText_input,save_button]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    load_container = widgets.HBox()\n",
    "    display(load_container)\n",
    "\n",
    "\n",
    "\n",
    "    loadFileText_input = widgets.Text(\n",
    "        value=\"topics\",\n",
    "        description='File Name:',\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "\n",
    "    load_button = widgets.Button(description=\"Load Topics\")\n",
    "    load_button.on_click(load_topics_ui)\n",
    "    load_container.children = [loadFileText_input,load_button]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    show_all_button = widgets.Button(description=\"Show Topics and Categories\")\n",
    "    show_all_button.on_click(lambda b: show_topics_and_categories())\n",
    "    display(show_all_button)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    classify_button = widgets.Button(description=\"Do Classification\")\n",
    "    classify_button.on_click(lambda b: do_Classification_Button_Function())\n",
    "    display(classify_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "setModel(\"openai-community/gpt2\",newInferenceType=\"transformers\",newModelType=\"Transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 'Toyota' (ID=d) added to topic 'A'.\n",
      "Topic 1 (ID=A): Car Brands\n",
      "  Prompt: INSTRUCTION: You are a helpful classifier. You select the correct of the possible categories for classifying a piece of text. The topic of the classification is '[TOPIC]'. The allowed categories are '[CATEGORIES]'. QUESTION: The text to be classified is '[TEXT]'. ANSWER: The correct category for this text is '\n",
      "    1. BMW (ID=a)\n",
      "    2. Audi (ID=b)\n",
      "    3. Mercedes (ID=c)\n",
      "    4. Toyota (ID=d)\n"
     ]
    }
   ],
   "source": [
    "add_topic(\n",
    "    topic_name=\"Car Brands\",\n",
    "    categories=[\"BMW\", \"Audi\", \"Mercedes\"]\n",
    ")\n",
    "\n",
    "# Add another category to the existing topic\n",
    "add_category(\"A\", categoryName=\"Toyota\")\n",
    "\n",
    "# Display all defined topics and their categories\n",
    "show_topics_and_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car Brands : BMW  (Relative Probability: 0.7008519067887374 )\n",
      "Classification results: ['BMW']\n",
      "Confidence scores: [0.7008519067887374]\n"
     ]
    }
   ],
   "source": [
    "text_to_classify = \"The new BMW X5 has impressive features.\"\n",
    "results, probabilities = classify(\n",
    "    text=text_to_classify,\n",
    "    isItASingleClassification=True,  # Print results to console\n",
    "    constrainedOutput=True  # Use constrained output mode\n",
    ")\n",
    "\n",
    "print(f\"Classification results: {results}\")\n",
    "print(f\"Confidence scores: {probabilities}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
